{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import Pixt_Dataset\n",
    "from dataset.transform import Pixt_ImageTransform\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image\n",
    "\n",
    "import random\n",
    "import clip\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train dataset class instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_dir = \"./data/\"\n",
    "annotation_dir = \"./data/annotation/annotation_merged_remove_gap.csv\"\n",
    "classes_ko_dir = \"./data/annotation/all_class_list_ko.pt\"\n",
    "classes_en_dir = \"./data/annotation/all_class_list_en.pt\"\n",
    "image_transform = Pixt_ImageTransform()\n",
    "train_dataset = Pixt_Dataset(img_dir, annotation_dir, image_transform)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train dataloader class instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(samples):\n",
    "    input_data = {}\n",
    "    input_data[\"image_tensor\"] = torch.stack([sample['image_tensor'] for sample in samples], dim=0)\n",
    "    input_data[\"text_ko\"] = [sample['text_ko'] for sample in samples]\n",
    "    return input_data\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    shuffle=False,\n",
    "    drop_last=True,\n",
    "    num_workers=0,\n",
    "    batch_size=16,\n",
    "    persistent_workers=False,\n",
    "    collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_ko_dir = \"./data/annotation/annotation_merged_remove_gap_class_ko.pt\"\n",
    "classes_en_dir = \"./data/annotation/annotation_merged_remove_gap_class_en.pt\"\n",
    "\n",
    "tags_ko_all_list = torch.load(classes_ko_dir)\n",
    "tags_en_all_list = torch.load(classes_en_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "model, _ = clip.load(\"RN50\", device=device)\n",
    "model.load_state_dict((torch.load(\"model.pt\", map_location=\"cuda\")))\n",
    "\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=5e-5,betas=(0.9,0.98),eps=1e-6,weight_decay=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_text_and_target_tensor(text_ko: list[list], max_length: int) -> list[torch.Tensor]:\n",
    "    # true label인 tag 담기\n",
    "    text_input_ko_list = []\n",
    "    for tags_ko in text_ko: # batch size 만큼 iteration\n",
    "        for tag_ko in tags_ko: # data sample 당 tag 개수만큼 iteration\n",
    "            text_input_ko_list.append(tag_ko)\n",
    "    text_input_ko_list = list(set(text_input_ko_list))\n",
    "    \n",
    "    # false label with random sampling 인 tag 담기\n",
    "    while True:\n",
    "        random_sample = random.sample(tags_ko_all_list, 1)[0]\n",
    "        if random_sample not in text_input_ko_list:\n",
    "            text_input_ko_list.append(random_sample)\n",
    "        if len(text_input_ko_list) == max_length:\n",
    "            break\n",
    "    \n",
    "    # 한국어(text_input_ko_list)에서 영어(text_input_en_list)로 번역하기\n",
    "    text_input_en_list = [tags_en_all_list[tags_ko_all_list.index(tag_ko)] for tag_ko in text_input_ko_list]\n",
    "    # tokenize 수행 및 tensor 변환\n",
    "    text_tensor = torch.cat([clip.tokenize(f\"a photo of a {c}\") for c in text_input_en_list])\n",
    "\n",
    "    # 한국어(text_ko)에서 영어(text_en)로 번역하기\n",
    "    text_en = []\n",
    "    for tags_ko in text_ko: # batch size 만큼 iteration\n",
    "        tmp = []\n",
    "        for tag_ko in tags_ko: # data sample 당 tag 개수만큼 iteration\n",
    "            tmp.append(tags_en_all_list[tags_ko_all_list.index(tag_ko)])\n",
    "        text_en.append(tmp)\n",
    "\n",
    "    # target tensor 생성\n",
    "    target_tensor_list = []\n",
    "    for tags_en in text_en:\n",
    "        target_tensor = torch.zeros_like(torch.empty(max_length))\n",
    "        for tag_en in tags_en:\n",
    "            if tag_en in text_input_en_list:\n",
    "                target_tensor[text_input_en_list.index(tag_en)] = 1\n",
    "        target_tensor_list.append(target_tensor)\n",
    "    target_tesnor = torch.stack(target_tensor_list, dim=0)\n",
    "    \n",
    "    return text_tensor, target_tesnor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 372/372 [33:39<00:00,  5.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 1 loss : 66.49728951402889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 372/372 [43:03<00:00,  6.95s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 2 loss : 63.715391261603244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 372/372 [37:59<00:00,  6.13s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 3 loss : 62.965434330765916\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 372/372 [42:13<00:00,  6.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 4 loss : 62.50462743287446\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 372/372 [44:06<00:00,  7.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 5 loss : 62.22277791013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "max_length = 500\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    one_epoch_loss = []\n",
    "    for batch in tqdm(train_dataloader):\n",
    "        image_tensor = batch[\"image_tensor\"].to(device) # torch.Tensor (16 x 3 x 224 x 224)\n",
    "        text_ko = batch[\"text_ko\"] # list[list] (16 x 가변적)\n",
    "        text_tensor, target_tensor = _get_text_and_target_tensor(text_ko, max_length)\n",
    "        text_tensor = text_tensor.to(device)\n",
    "        target_tensor = target_tensor.to(device)\n",
    "\n",
    "        logits_per_image, logits_per_text = model(image_tensor, text_tensor)\n",
    "        loss = (loss_func(logits_per_image, target_tensor) + loss_func(logits_per_text.T, target_tensor)) / 2\n",
    "        one_epoch_loss.append(loss.item())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(\"epoch :\", epoch+1, \"loss :\", sum(one_epoch_loss) / len(one_epoch_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),\"model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/dataset3/110.webp\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([5691, 77])\n",
      "\n",
      "Top predictions:\n",
      "\n",
      "          spring: 3.99%\n",
      "          nature: 2.68%\n",
      "     countryside: 2.31%\n",
      "           sight: 2.20%\n",
      "           seoul: 1.89%\n",
      "           water: 1.54%\n",
      "    spring water: 1.52%\n",
      "            pine: 1.34%\n",
      "               0: 1.34%\n",
      "         objects: 1.22%\n"
     ]
    }
   ],
   "source": [
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# model, preprocess = clip.load('RN50', device)\n",
    "# model.load_state_dict((torch.load(\"model.pt\", map_location=\"cuda\")))\n",
    "\n",
    "classes_list = torch.load(\"./data/annotation/annotation_merged_remove_gap_class_en.pt\")\n",
    "classes_list = [tag_ko.lower() for tag_ko in classes_list]\n",
    "classes_list = sorted(set(classes_list))\n",
    "text_input = torch.cat([clip.tokenize(f\"a photo of a {c}\") for c in classes_list]).to(device)\n",
    "\n",
    "image_number = 110\n",
    "file_path = \"./data/dataset3/\"+ str(image_number) + \".webp\"\n",
    "print(file_path)\n",
    "Image.open(file_path).show()\n",
    "\n",
    "Image_transform = Pixt_ImageTransform()\n",
    "image_input = Image_transform(Image.open(file_path).convert(\"RGB\")).float().unsqueeze(0).to(device)\n",
    "print(image_input.shape, text_input.shape)\n",
    "\n",
    "with torch.no_grad():\n",
    "    image_features = model.encode_image(image_input)\n",
    "    text_features = model.encode_text(text_input)\n",
    "\n",
    "image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "values, indices = similarity[0].topk(10)\n",
    "\n",
    "print(\"\\nTop predictions:\\n\")\n",
    "for value, index in zip(values, indices):\n",
    "    print(f\"{classes_list[index]:>16s}: {100 * value.item():.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pixt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
