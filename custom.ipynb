{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.1+cu117\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import csv\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_dir='/home/irteam/junghye-dcloud-dir/Pixt/data/dataset/dataset1'\n",
    "label_dir='/home/irteam/junghye-dcloud-dir/Pixt/data/dataset/dataset1/motion_excluded.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sol2\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "  def __init__(self,img_dir:str,label_dir:str,transform):\n",
    "  \n",
    "    super().__init__\n",
    "    self.img_dir=img_dir\n",
    "    self.images=os.listdir(self.img_dir)\n",
    "    self.transform=transform\n",
    "    all_labels=[]\n",
    "    with open(label_dir,'r') as f:\n",
    "      data=csv.reader(f)\n",
    "      self.label_data=list(data)\n",
    "      # 전체 label\n",
    "      for row in self.label_data:\n",
    "        all_labels.extend(row[2:])\n",
    "\n",
    "      unique_labels=list(set(all_labels))\n",
    "\n",
    "    self.label_to_idx={unique_labels[i]:i+1 for i in range(len(unique_labels))}\n",
    "    \n",
    "\n",
    "\n",
    "  def __len__(self):\n",
    "    return (len(self.images))\n",
    "\n",
    "  def __getitem__(self,idx):\n",
    "    file_path=os.path.join(self.img_dir,self.images[idx])\n",
    "    img=Image.open(file_path).convert('RGB')\n",
    "    img_transformed=self.transform(img)\n",
    "\n",
    "    target_row=self.label_data[idx]\n",
    "    target_list=[self.label_to_idx[item] for item in target_row[2:]]\n",
    "      \n",
    "    target_tensor=torch.tensor(target_list).float()\n",
    "\n",
    "\n",
    "    return img_transformed,target_tensor\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform=transforms.Compose([\n",
    "    transforms.Resize((256, 256)),          # 개와 고양이 사진 파일의 크기가 다르므로, Resize로 맞춰줍니다.\n",
    "    transforms.CenterCrop((224, 224)),      # 중앙 Crop\n",
    "    transforms.RandomHorizontalFlip(0.5),   # 50% 확률로 Horizontal Flip\n",
    "    transforms.ToTensor(), \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(img_dir,label_dir,train_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 224, 224]) torch.Size([15])\n"
     ]
    }
   ],
   "source": [
    "x,y=train_dataset.__getitem__(3)\n",
    "print(x.shape,y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batchDummy): \n",
    "\n",
    "  images_list,label_list,=[],[]\n",
    "\n",
    "  for (_img,_label) in batchDummy:\n",
    "    images_list.append(_img)\n",
    "    label_list.append(_label)\n",
    "  label_list=pad_sequence(label_list,batch_first=True,padding_value=0)\n",
    "  return images_list,label_list\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True,collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_data: torch.Size([3, 224, 224])\n",
      "sample_label torch.Size([28])\n",
      "sample_data: torch.Size([3, 224, 224])\n",
      "sample_label torch.Size([28])\n",
      "sample_data: torch.Size([3, 224, 224])\n",
      "sample_label torch.Size([28])\n",
      "sample_data: torch.Size([3, 224, 224])\n",
      "sample_label torch.Size([28])\n",
      "sample_data: torch.Size([3, 224, 224])\n",
      "sample_label torch.Size([28])\n",
      "sample_data: torch.Size([3, 224, 224])\n",
      "sample_label torch.Size([28])\n",
      "sample_data: torch.Size([3, 224, 224])\n",
      "sample_label torch.Size([28])\n",
      "sample_data: torch.Size([3, 224, 224])\n",
      "sample_label torch.Size([28])\n",
      "sample_data: torch.Size([3, 224, 224])\n",
      "sample_label torch.Size([28])\n",
      "sample_data: torch.Size([3, 224, 224])\n",
      "sample_label torch.Size([28])\n",
      "sample_data: torch.Size([3, 224, 224])\n",
      "sample_label torch.Size([28])\n",
      "sample_data: torch.Size([3, 224, 224])\n",
      "sample_label torch.Size([28])\n",
      "sample_data: torch.Size([3, 224, 224])\n",
      "sample_label torch.Size([28])\n",
      "sample_data: torch.Size([3, 224, 224])\n",
      "sample_label torch.Size([28])\n",
      "sample_data: torch.Size([3, 224, 224])\n",
      "sample_label torch.Size([28])\n",
      "sample_data: torch.Size([3, 224, 224])\n",
      "sample_label torch.Size([28])\n",
      "sample_data: torch.Size([3, 224, 224])\n",
      "sample_label torch.Size([28])\n",
      "sample_data: torch.Size([3, 224, 224])\n",
      "sample_label torch.Size([28])\n",
      "sample_data: torch.Size([3, 224, 224])\n",
      "sample_label torch.Size([28])\n",
      "sample_data: torch.Size([3, 224, 224])\n",
      "sample_label torch.Size([28])\n",
      "sample_data: torch.Size([3, 224, 224])\n",
      "sample_label torch.Size([28])\n",
      "sample_data: torch.Size([3, 224, 224])\n",
      "sample_label torch.Size([28])\n",
      "sample_data: torch.Size([3, 224, 224])\n",
      "sample_label torch.Size([28])\n",
      "sample_data: torch.Size([3, 224, 224])\n",
      "sample_label torch.Size([28])\n",
      "sample_data: torch.Size([3, 224, 224])\n",
      "sample_label torch.Size([28])\n",
      "sample_data: torch.Size([3, 224, 224])\n",
      "sample_label torch.Size([28])\n",
      "sample_data: torch.Size([3, 224, 224])\n",
      "sample_label torch.Size([28])\n",
      "sample_data: torch.Size([3, 224, 224])\n",
      "sample_label torch.Size([28])\n",
      "sample_data: torch.Size([3, 224, 224])\n",
      "sample_label torch.Size([28])\n",
      "sample_data: torch.Size([3, 224, 224])\n",
      "sample_label torch.Size([28])\n",
      "sample_data: torch.Size([3, 224, 224])\n",
      "sample_label torch.Size([28])\n",
      "sample_data: torch.Size([3, 224, 224])\n",
      "sample_label torch.Size([28])\n"
     ]
    }
   ],
   "source": [
    "batch=next(iter(train_loader))\n",
    "\n",
    "data,labels=batch\n",
    "\n",
    "for i in range(len(data)):\n",
    "    sample_data=data[i]\n",
    "    sample_label=labels[i]\n",
    "\n",
    "    print('sample_data:',sample_data.shape)\n",
    "    print('sample_label',sample_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ipykernel",
   "language": "python",
   "name": "ipykernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
